{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yeshavyas/speech-emotion-recognition-ser?scriptVersionId=173187956\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport librosa\nimport re\n# \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \n# \nfrom sklearn import preprocessing, svm\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n# \nimport tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:13:18.333684Z","iopub.execute_input":"2024-03-12T17:13:18.334204Z","iopub.status.idle":"2024-03-12T17:13:30.715141Z","shell.execute_reply.started":"2024-03-12T17:13:18.334161Z","shell.execute_reply":"2024-03-12T17:13:30.713803Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"Ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nCrema = \"/kaggle/input/cremad/AudioWAV/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\nTess =\"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"","metadata":{"execution":{"iopub.status.busy":"2024-03-12T04:56:53.588984Z","iopub.execute_input":"2024-03-12T04:56:53.589528Z","iopub.status.idle":"2024-03-12T04:56:53.595612Z","shell.execute_reply.started":"2024-03-12T04:56:53.589502Z","shell.execute_reply":"2024-03-12T04:56:53.594795Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nemotion_df = []\n\nfor dir in ravdess_directory_list:\n    actor = os.listdir(os.path.join(Ravdess, dir))\n    for wav in actor:\n        info = wav.partition(\".wav\")[0].split(\"-\")\n        emotion = int(info[2])\n        emotion_df.append((emotion, os.path.join(Ravdess, dir, wav)))\n\nRavdess_df = pd.DataFrame.from_dict(emotion_df)\nRavdess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nRavdess_df.Emotion.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T04:57:11.786652Z","iopub.execute_input":"2024-03-12T04:57:11.786954Z","iopub.status.idle":"2024-03-12T04:57:12.602551Z","shell.execute_reply.started":"2024-03-12T04:57:11.786931Z","shell.execute_reply":"2024-03-12T04:57:12.60191Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    Emotion                                               Path\n0  surprise  /kaggle/input/ravdess-emotional-speech-audio/a...\n1   neutral  /kaggle/input/ravdess-emotional-speech-audio/a...\n2   disgust  /kaggle/input/ravdess-emotional-speech-audio/a...\n3   disgust  /kaggle/input/ravdess-emotional-speech-audio/a...\n4   neutral  /kaggle/input/ravdess-emotional-speech-audio/a...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>surprise</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"emotion_df = []\n\nfor wav in os.listdir(Crema):\n    info = wav.partition(\".wav\")[0].split(\"_\")\n    if info[2] == 'SAD':\n        emotion_df.append((\"sad\", Crema + \"/\" + wav))\n    elif info[2] == 'ANG':\n        emotion_df.append((\"angry\", Crema + \"/\" + wav))\n    elif info[2] == 'DIS':\n        emotion_df.append((\"disgust\", Crema + \"/\" + wav))\n    elif info[2] == 'FEA':\n        emotion_df.append((\"fear\", Crema + \"/\" + wav))\n    elif info[2] == 'HAP':\n        emotion_df.append((\"happy\", Crema + \"/\" + wav))\n    elif info[2] == 'NEU':\n        emotion_df.append((\"neutral\", Crema + \"/\" + wav))\n    else:\n        emotion_df.append((\"unknown\", Crema + \"/\" + wav))\n\n\nCrema_df = pd.DataFrame.from_dict(emotion_df)\nCrema_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nCrema_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T04:57:15.497677Z","iopub.execute_input":"2024-03-12T04:57:15.49798Z","iopub.status.idle":"2024-03-12T04:57:15.97719Z","shell.execute_reply.started":"2024-03-12T04:57:15.497958Z","shell.execute_reply":"2024-03-12T04:57:15.976378Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Emotion                                               Path\n0  disgust  /kaggle/input/cremad/AudioWAV//1028_TSI_DIS_XX...\n1    happy  /kaggle/input/cremad/AudioWAV//1075_IEO_HAP_LO...\n2    happy  /kaggle/input/cremad/AudioWAV//1084_ITS_HAP_XX...\n3  disgust  /kaggle/input/cremad/AudioWAV//1067_IWW_DIS_XX...\n4  disgust  /kaggle/input/cremad/AudioWAV//1066_TIE_DIS_XX...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>disgust</td>\n      <td>/kaggle/input/cremad/AudioWAV//1028_TSI_DIS_XX...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>happy</td>\n      <td>/kaggle/input/cremad/AudioWAV//1075_IEO_HAP_LO...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>happy</td>\n      <td>/kaggle/input/cremad/AudioWAV//1084_ITS_HAP_XX...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/cremad/AudioWAV//1067_IWW_DIS_XX...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>disgust</td>\n      <td>/kaggle/input/cremad/AudioWAV//1066_TIE_DIS_XX...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nemotion_df = []\n\nfor dir in tess_directory_list:\n    for wav in os.listdir(os.path.join(Tess, dir)):\n        info = wav.partition(\".wav\")[0].split(\"_\")\n        emo = info[2]\n        if emo == \"ps\":\n            emotion_df.append((\"surprise\", os.path.join(Tess, dir, wav)))\n        else:\n            emotion_df.append((emo, os.path.join(Tess, dir, wav)))\n\n\nTess_df = pd.DataFrame.from_dict(emotion_df)\nTess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nTess_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T04:57:33.515023Z","iopub.execute_input":"2024-03-12T04:57:33.51534Z","iopub.status.idle":"2024-03-12T04:57:34.646791Z","shell.execute_reply.started":"2024-03-12T04:57:33.515316Z","shell.execute_reply":"2024-03-12T04:57:34.645944Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"  Emotion                                               Path\n0    fear  /kaggle/input/toronto-emotional-speech-set-tes...\n1    fear  /kaggle/input/toronto-emotional-speech-set-tes...\n2    fear  /kaggle/input/toronto-emotional-speech-set-tes...\n3    fear  /kaggle/input/toronto-emotional-speech-set-tes...\n4    fear  /kaggle/input/toronto-emotional-speech-set-tes...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"savee_directiory_list = os.listdir(Savee)\n\nemotion_df = []\n\nfor wav in savee_directiory_list:\n    info = wav.partition(\".wav\")[0].split(\"_\")[1].replace(r\"[0-9]\", \"\")\n    emotion = re.split(r\"[0-9]\", info)[0]\n    if emotion=='a':\n        emotion_df.append((\"angry\", Savee + \"/\" + wav))\n    elif emotion=='d':\n        emotion_df.append((\"disgust\", Savee + \"/\" + wav))\n    elif emotion=='f':\n        emotion_df.append((\"fear\", Savee + \"/\" + wav))\n    elif emotion=='h':\n        emotion_df.append((\"happy\", Savee + \"/\" + wav))\n    elif emotion=='n':\n        emotion_df.append((\"neutral\", Savee + \"/\" + wav))\n    elif emotion=='sa':\n        emotion_df.append((\"sad\", Savee + \"/\" + wav))\n    else:\n        emotion_df.append((\"surprise\", Savee + \"/\" + wav))\n\n\nSavee_df = pd.DataFrame.from_dict(emotion_df)\nSavee_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nSavee_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T04:57:36.469102Z","iopub.execute_input":"2024-03-12T04:57:36.469737Z","iopub.status.idle":"2024-03-12T04:57:36.636809Z","shell.execute_reply.started":"2024-03-12T04:57:36.469706Z","shell.execute_reply":"2024-03-12T04:57:36.635947Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Emotion                                               Path\n0    happy  /kaggle/input/surrey-audiovisual-expressed-emo...\n1     fear  /kaggle/input/surrey-audiovisual-expressed-emo...\n2    happy  /kaggle/input/surrey-audiovisual-expressed-emo...\n3  disgust  /kaggle/input/surrey-audiovisual-expressed-emo...\n4    angry  /kaggle/input/surrey-audiovisual-expressed-emo...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>happy</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fear</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>happy</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>angry</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"path_df = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0, ignore_index=True)\nprint(path_df.shape)\nfeatures_path = \"./all_datasets_path_emotions.csv\"\npath_df.to_csv(features_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T04:58:31.18698Z","iopub.execute_input":"2024-03-12T04:58:31.187257Z","iopub.status.idle":"2024-03-12T04:58:31.222199Z","shell.execute_reply.started":"2024-03-12T04:58:31.187236Z","shell.execute_reply":"2024-03-12T04:58:31.221252Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(12162, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"n_fft = 2048\nhop_length = 512\n\ndef chunks(data, frame_length, hop_length):\n    for i in range(0, len(data), hop_length):\n        yield data[i:i+frame_length]\n\n# Zero Crossing Rate\ndef zcr(data, frame_length=2048, hop_length=512):\n    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(zcr)\n\n\ndef energy(data, frame_length=2048, hop_length=512):\n    en = np.array([np.sum(np.power(np.abs(data[hop:hop+frame_length]), 2)) for hop in range(0, data.shape[0], hop_length)])\n    return en / frame_length\n\n\ndef rmse(data, frame_length=2048, hop_length=512):\n    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(rmse)\n\n\ndef entropy_of_energy(data, frame_length=2048, hop_length=512):\n    energies = energy(data, frame_length, hop_length)\n    energies /= np.sum(energies)\n\n    entropy = 0.0\n    entropy -= energies * np.log2(energies)\n    return entropy\n\n\ndef spc(data, sr, frame_length=2048, hop_length=512):\n    spectral_centroid = librosa.feature.spectral_centroid(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(spectral_centroid)\n\ndef spc_flux(data):\n    isSpectrum = data.ndim == 1\n    if isSpectrum:\n        data = np.expand_dims(data, axis=1)\n\n    X = np.c_[data[:, 0], data]\n    af_Delta_X = np.diff(X, 1, axis=1)\n    vsf = np.sqrt((np.power(af_Delta_X, 2).sum(axis=0))) / X.shape[0]\n\n    return np.squeeze(vsf) if isSpectrum else vsf\n\n\ndef spc_rollof(data, sr, frame_length=2048, hop_length=512):\n    spcrollof = librosa.feature.spectral_rolloff(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(spcrollof)\n\n\ndef chroma_stft(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = librosa.feature.chroma_stft(S=stft, sr=sr)\n    return np.squeeze(chroma_stft.T) if not flatten else np.ravel(chroma_stft.T)\n\ndef chroma_cens(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    chroma_cens = librosa.feature.chroma_cens(y=x, sr=sr)\n    return np.squeeze(chroma_cens.T) if not flatten else np.ravel(chroma_cens.T)\n    \ndef mel_spc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mel = librosa.feature.melspectrogram(y=data, sr=sr)\n    return np.squeeze(mel.T) if not flatten else np.ravel(mel.T)\n\ndef mfcc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mfcc_feature = librosa.feature.mfcc(y=data, sr=sr)\n    return np.squeeze(mfcc_feature.T) if not flatten else np.ravel(mfcc_feature.T)\n\ndef spec_bandwidth(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    spec_btw  = librosa.feature.spectral_bandwidth(y=data, sr=sr)\n    return np.squeeze(spec_btw)\n\ndef spec_contrast(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    stft = np.abs(librosa.stft(data))\n    spec_cont = librosa.feature.spectral_contrast(S=stft,sr=sr)\n    return np.squeeze(spec_cont.T) if not flatten else np.ravel(spec_cont.T)\n\ndef spec_flat(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    spec_f = librosa.feature.spectral_flatness(y=data)\n    return np.squeeze(spec_f)\n\n\n# for a single audio file\npath = np.array(path_df[\"Path\"])[658]\ndata, sampling_rate = librosa.load(path, duration=2.5, offset=0.6)\nlen(data)\n\nprint(\"ZCR: \", zcr(data).shape)\nprint(\"Energy: \", energy(data).shape)\nprint(\"Entropy of Energy :\", entropy_of_energy(data).shape)\nprint(\"RMS :\", rmse(data).shape)\nprint(\"Spectral Centroid :\", spc(data, sampling_rate).shape)\n# print(\"Spectral Entropy: \", spc_entropy(data, sampling_rate).shape)\nprint(\"Spectral Flux: \", spc_flux(data).shape)\nprint(\"Spectral Rollof: \", spc_rollof(data, sampling_rate).shape)\nprint(\"Chroma STFT: \", chroma_stft(data, sampling_rate).shape)\nprint(\"MelSpectrogram: \", mel_spc(data, sampling_rate).shape)\nprint(\"MFCC: \", mfcc(data, sampling_rate).shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# functions for augmentation\ndef noise(data, random=False, rate=0.035, threshold=0.075):\n    \"\"\"Add some noise to sound sample. Use random if you want to add random noise with some threshold.\n    Or use rate Random=False and rate for always adding fixed noise.\"\"\"\n    if random:\n        rate = np.random.random() * threshold\n    noise_amp = rate*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    \"\"\"Stretching data with some rate.\"\"\"\n    return librosa.effects.time_stretch(data, rate=rate)\n\ndef shift(data, rate=1000):\n    \"\"\"Shifting data with some rate\"\"\"\n    shift_range = int(np.random.uniform(low=-5, high = 5)*rate)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7, random=False):\n    \"\"\"\"Add some pitch to sound sample. Use random if you want to add random pitch with some threshold.\n    Or use pitch_factor Random=False and rate for always adding fixed pitch.\"\"\"\n    if random:\n        pitch_factor=np.random.random() * pitch_factor\n    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T14:01:27.382132Z","iopub.execute_input":"2024-04-21T14:01:27.382564Z","iopub.status.idle":"2024-04-21T14:01:27.427594Z","shell.execute_reply.started":"2024-04-21T14:01:27.382521Z","shell.execute_reply":"2024-04-21T14:01:27.42567Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def extract_features(data, sr, frame_length=2048, hop_length=512):\n    result = np.array([])\n    result = np.hstack((result,\n                        zcr(data, frame_length, hop_length),\n                        # np.mean(energy(data, frame_length, hop_length),axis=0),\n                        # np.mean(entropy_of_energy(data, frame_length, hop_length), axis=0),\n                        rmse(data, frame_length, hop_length),\n                        # spc(data, sr, frame_length, hop_length),\n                        # spc_entropy(data, sr),\n                        # spc_flux(data),\n                        # spc_rollof(data, sr, frame_length, hop_length),\n                        # chroma_stft(data, sr, frame_length, hop_length),\n                        # mel_spc(data, sr, frame_length, hop_length, flatten=True)\n                        mfcc(data, sr, frame_length, hop_length)\n                                    ))\n    return result\n\ndef get_features(path, duration=2.5, offset=0.6):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=duration, offset=offset)\n\n     # without augmentation\n    res1 = extract_features(data, sample_rate)\n    result = np.array(res1)\n\n    # data with noise\n    noise_data = noise(data, random=True)\n    res2 = extract_features(noise_data, sample_rate)\n    result = np.vstack((result, res2)) # stacking vertically\n\n    # data with pitching\n    pitched_data = pitch(data, sample_rate, random=True)\n    res3 = extract_features(pitched_data, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    # data with pitching and white_noise\n    new_data = pitch(data, sample_rate, random=True)\n    data_noise_pitch = noise(new_data, random=True)\n    res3 = extract_features(data_noise_pitch, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    return result\n\nX, Y = [], []\nprint(\"Feature processing...\")\nfor path, emotion, ind in zip(path_df.Path, path_df.Emotion, range(path_df.Path.shape[0])):\n    features = get_features(path)\n    if ind % 100 == 0:\n        print(f\"{ind} samples has been processed...\")\n    for ele in features:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\nprint(\"Done.\")\nfeatures_path = \"./zcr_rmse_mfcc_plus_random_aug_features.csv\"\nextracted_df = pd.DataFrame(X)\nextracted_df[\"labels\"] = Y\nextracted_df.to_csv(features_path, index=False)\nextracted_df.head()\n# extracted_df = pd.read_csv(features_path)\n# print(extracted_df.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_path = \"./zcr_rmse_mfcc_plus_random_aug_features.csv\"\n\ndf = pd.read_csv(features_path)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# specific for ravdess\n# ser_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust',8:'surprise'},inplace=True)\n \ny = np.array(df['labels'])\nl_encode = LabelEncoder()\nl_encode.fit(y)\ny = l_encode.transform(y)\ny.shape\ndf.drop(columns=['labels'], inplace=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.fillna(0)\n\nscaler = preprocessing.StandardScaler()\nX_scaled = scaler.fit_transform(df.values)\nX_scaled.shape\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=30)\n\nX_cv, X_test, y_cv, y_test = train_test_split(X_test, y_test, test_size=0.2, random_state=30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_performance(clf, X_train=X_train, y_train=y_train, X_cv=X_cv, y_cv=y_cv):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_cv)\n    accuracy = accuracy_score(y_cv, y_pred)\n    report = classification_report(y_cv, y_pred)\n    other_performance_params = precision_recall_fscore_support(y_cv, y_pred, average=\"macro\")\n\n    return accuracy, report, other_performance_params\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # using logistic regression\n# clf6 = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=1000)\n# # using decision tree \n# clf4 = DecisionTreeClassifier()\n\n# Using support vector machine classifier\nclf = svm.SVC(C=7)\n\n# using k-nearest-neighbours\n# clf5 = KNeighborsClassifier()\n\n# #  using bagging ensemble learning\n# clf3 = BaggingClassifier(estimator=svm.SVC(C=7), n_estimators=20, random_state=0)\n\n# using boosting ensemble\n# clf1 = HistGradientBoostingClassifier(max_iter=100)\n# # using voting \n# clf = VotingClassifier(estimators=[('histgr', clf1), ('svc', clf2),('bagclf', clf3), ('dt', clf4), ('knn',clf5)], voting='hard')\n\n\n# # using stacking ensemble learning\n# def get_stacking():\n#     # define the base models\n#     level0 = list()\n\n#     level0.append(('knn', KNeighborsClassifier()))\n#     level0.append(('svc', svm.SVC(C=7)))\n# #     level0.append(('logreg', LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=1000)))\n#     level0.append(('bagclf', BaggingClassifier(estimator=svm.SVC(C=7),n_estimators=20, random_state=0)))\n# #     level0.append(('vboostclf', VotingClassifier(estimators=[('hgbc', clf1), ('svc', clf2), ('bgclf', clf3)], voting='hard')))\n#     # define meta learner model\n#     level1 = svm.SVC(C=7)\n#     # define the stacking ensemble\n#     model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n#     return model\n\n# clf = get_stacking()\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# universal to all sklearn models\ntest_accuracy, report, other_performance_params = get_model_performance(clf)\ntrain_accuracy = clf.score(X_train, y_train)\nprint(f\"{type(clf).__name__}\\n Test Accuracy: {test_accuracy} \\n Train Accuracy: {train_accuracy}\\n Classification report \\n{report} \\n Precision Recall fscore support \\n {other_performance_params}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# harmonic and percussive components \nfrom tqdm import tqdm\nimport librosa as lb\ndef feature_extractor_harmonic_percussive(path):\n    data, simple_rate = lb.load(path)\n    D = lb.stft(data)\n    #melspec_harmonic, melspec_percussive = lb.decompose.hpss(D)\n    #melspec_harmonic = np.mean(melspec_harmonic)\n    #melspec_percussive = np.mean(melspec_percussive)\n    #logspec_hp = np.average([melspec_harmonic, melspec_percussive])\n    harmonic = lb.effects.harmonic(data,margin=5.0)\n    percussive = librosa.effects.percussive(data, margin=5.0)\n    #harmonic = np.mean(harmonic)\n    #percussive = np.mean(percussive)\n    logspec_hp = np.average([harmonic, percussive])\n    return logspec_hp\n\ndef feature_extractor_logmelspectrogram(path):\n    data, simple_rate = lb.load(path)\n    data = lb.feature.melspectrogram(data)\n    data = lb.power_to_db(data)\n    data = np.mean(data,axis=1)\n    return data\n\n\n\nx_harmonic_percussive, x_logmelspectrogram, y = [], [], []\nfor path in tqdm(files):\n    file_name = path.split('\\\\')[-1] #Return File Name\n    class_label = path.split('-')[-1] #Return Class Id\n    class_label = class_label.split('.')[0]\n        \n    x_harmonic_percussive.append(feature_extractor_harmonic_percussive(path))\n    x_logmelspectrogram.append(feature_extractor_logmelspectrogram(path))\n    y.append(class_label)\n\nx_harmonic_percussive = np.array(x_harmonic_percussive)\nx_logmelspectrogram = np.array(x_logmelspectrogram)\ny = np.array(y)\nfeatures = np.hstack((x_harmonic_percussive.reshape(len(x_harmonic_percussive), 1), x_logmelspectrogram))\nlabels = y\nfeatures.shape, labels.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using deep learning models\ndef plot_history(network_history):\n    plt.figure()\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.plot(network_history.history['loss'])\n    plt.plot(network_history.history['val_loss'])\n    plt.legend(['Training','Validation'])\n    \n    plt.figure()\n    plt.xlabel('Epochs')\n    plt.ylabel('Acc')\n    plt.plot(network_history.history['acc'])\n    plt.plot(network_history.history['val_acc'])\n    plt.legend(['Training','Validation'])\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.backend as K\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Conv1D(512, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\",\n                        input_shape=(X_train.shape[1], 1),\n                        # kernel_regularizer=tf.keras.regularizers.L2(0.001)\n                        ))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(tf.keras.layers.Conv1D(512, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\", \n                        # kernel_regularizer=tf.keras.regularizers.L2(0.001)\n                                ))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(tf.keras.layers.Conv1D(256, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\",\n                                 # kernel_regularizer=tf.keras.regularizers.L2(0.001)\n                                ))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(tf.keras.layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu', \n                                 # kernel_regularizer=tf.keras.regularizers.L2(0.001)\n                                ))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu', \n                                 # kernel_regularizer=tf.keras.regularizers.L2(0.001)\n                                ))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same'))\n\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(512, activation='relu', \n                                # kernel_regularizer=tf.keras.regularizers.L2(0.001)\n                               ))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(7, activation=\"softmax\"))\n\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\", f1_m])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_acc\",\n    mode='auto',\n    patience=5,\n    restore_best_weights=True\n)\nreduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_acc',\n    patience=3,\n    verbose=1,\n    factor=0.5,\n    min_lr=0.00001\n)\n\nEPOCHS = 100\nbatch_size = 64\nhistory = model.fit(\n    X_train,\n    tf.keras.utils.to_categorical(y_train),\n    validation_data=(X_cv, tf.keras.utils.to_categorical(y_cv)),\n    epochs=EPOCHS,\n    batch_size=batch_size,\n    callbacks=[early_stopping_callback, reduce_lr_callback]\n)\nplot_history(history)\nprint(\"Accuracy of our model on test data : \" , model.evaluate(X_test,tf.keras.utils.to_categorical(y_test))[1]*100 , \"%\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,tf.keras.utils.to_categorical(y_test))[1]*100 , \"%\")\n\ny_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)\ny_pred\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_check = np.argmax(tf.keras.utils.to_categorical(y_test), axis=1)\ny_check","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=y_check, y_pred=y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm_plot_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}